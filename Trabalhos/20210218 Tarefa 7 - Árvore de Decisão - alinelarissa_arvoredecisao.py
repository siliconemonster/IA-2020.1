# -*- coding: utf-8 -*-
"""AlineLarissa_ArvoreDecisao.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12pO8-gmRuBOOb9XR3K2FdhtmHsyYMZC3

O objetivo desta tarefa é realizar um conjunto de experimentos usando o método de árvore de decisão e um dataset escolhido no site kaggle (https://www.kaggle.com/datasets).

Deve ser entregue no classroom um relatório contendo as seguintes informações:

1- descrição do dataset selecionado: informe o número de instâncias do dataset, os atributos, assim como seus respectivos tipos e os valores que cada atributo pode assumir;

2- defina um experimento base: informe quais atributos do dataset serão considerados (você pode usar todos ou selecionar apenas alguns - neste caso justifique sua decisão); informe os parâmetros usados neste experimento (como o dataset será dividido em treinamento/teste; no caso de usar k-fold, qual o valor de k foi escolhido, qual função será usada para escolher o atributo durante a construção da árvore - gini ou entropia; como é feita a avaliação da árvore gerada que será, etc ). 

3- explore alternativas para melhorar os resultados obtidos: faça isso mudando os parâmetros usados no experimento base e/ou fazendo podas na árvore resultante. Justifique a alteração e o que você esperava que acontecesse quando decidiu fazer a alteração do parâmetro e qual o resultado obtido. Compare seus resultados com o experimento base.

4- compare os resultados que você obteve na base que você escolheu com os obtidos por outros métodos e que estejam disponíveis no kaggle.

Os códigos usados nos seus experimentos também devem ser entregues.

O trabalho pode ser feito em dupla.

Árvore de Decisão

Inicialmente são carregadas as bibliotecas necessárias: 

+ pandas (https://pandas.pydata.org/): biblioteca para análise de dados em python
+ scikit-learn (https://scikit-learn.org/stable/): biblioteca de ferramentas de aprendizagem de máquina em python
+ Base do Kagle: https://www.kaggle.com/heptapod/titanic
"""

# Carregando Bibliotecas
import pandas as pd
from sklearn.tree import DecisionTreeClassifier # Importa o classificador de árvore de decisão
from sklearn import metrics #Importa métrica para calcular acurácia - módulo do scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
from IPython.display import Image  
import pydotplus
import numpy as np
from sklearn.model_selection import KFold

from google.colab import drive
drive.mount('/content/drive')

def defineConceito(dataset):
  # definir o conceito alvo e as features usadas

  feature_cols = ['Passengerid', 'Age', 'Fare', 'Sex', 'sibsp', 'Parch', 'Pclass', 'Embarked'] # features (atributos) que serão usadas no aprendizado

  X = dataset[feature_cols] # selecionamos as colunas correspondentes aos atributos que serão usados
  y = dataset.survived # conceito que queremos aprender. Selecionamos a coluna com a classificação das instâncias

  return X, y

"""Vamos criar agora o nosso classificador de árvore de decisão, usando como função para seleção do atributo a entropia:"""

def separaTeste(clf, Xone_hot_data, y, tam_teste):

  X_train, X_test, Y_train, Y_test = train_test_split(Xone_hot_data, y, test_size=tam_teste, shuffle=False)
  #print("Treino:")
  #display(Y_train.to_frame())
  #print("Teste:")
  #display(Y_test.to_frame())

  return X_train, X_test, Y_train, Y_test

"""Podemos visualizar a árvore de decisão gerada usando o pacote graphviz:
https://graphviz.org/
https://pypi.org/project/graphviz/
"""

def plotaArvore(clf, Xone_hot_data):
  # tem que usar feature_names = one_hot_data.columns pois feature_names = feature_cols tem menos atributos
  # pois o one-hot acrescenta mais
  dot_data = StringIO()
  export_graphviz(clf, out_file=dot_data,  
                  filled=True, rounded=True,
                  special_characters=True,feature_names = Xone_hot_data.columns,class_names=['0', '1'])
  graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
  graph.write_png('arvore1.png')
  arvore = graph.create_png()

  return arvore

"""Podemos medir a acurácia testando quantas instâncias são classificadas corretamente. Observe que usamos todas as instâncias disponíveis no treinamento. A acurácia obtida foi 1, indicando que classificamos corretamente todos os exemplos de nosso conjunto."""

def calculaAcuracia(X_test, Y_test, clf):
  # Usando modelo para classificar os dados que temos a disposição
  y_pred = clf.predict(X_test)
  # Medida de acuracia, que indica quantas instâncias são corretamente classificadas
  acuracia = metrics.accuracy_score(Y_test, y_pred)

  return acuracia

def calculaMediaAcuracias(acuracias):
  soma = sum(acuracias)
  media = soma/len(acuracias)

  return media

def fazKfold(num_folds, Xone_hot_data, clf, y):
  kf = KFold(n_splits=num_folds, shuffle=False)
  Xone_array = Xone_hot_data.to_numpy()
  acuracias = []
  for train_index, test_index in kf.split(Xone_array):
    X_train, X_test = Xone_array[train_index], Xone_array[test_index]
    #print(X_train)
    #print(X_test)
    Y_train, Y_test = y[train_index], y[test_index]
    #print(Y_train)
    #print(Y_test)
    clf = clf.fit(X_train, Y_train)
    acuracias.append(calculaAcuracia(X_test, Y_test, clf))

  media_kFold = calculaMediaAcuracias(acuracias)

  return media_kFold

#---- main -----
def main(criterio, tam_teste, num_folds):
  # carregando a base de dados a ser usada
  dataset = pd.read_excel("/content/drive/MyDrive/Trab_aprendizado/Trab_Titanic/titanic_survive.xlsx")
  #display(dataset)
  X, y = defineConceito(dataset)

  # "pd.get_dummies" que converte os atributos categóricos em atributos com valores 0 e 1
  Xone_hot_data = pd.get_dummies(X[['Passengerid', 'Age', 'Fare', 'Sex', 'sibsp', 'Parch', 'Pclass', 'Embarked']]) 
  clf = DecisionTreeClassifier(criterion=criterio)

  ## SEM O K FOLD ##
  X_train, X_test, Y_train, Y_test = separaTeste(clf, Xone_hot_data, y, tam_teste)
  clf = clf.fit(X_train, Y_train)
  arvore = plotaArvore(clf, Xone_hot_data)
  acuracia = calculaAcuracia(X_test, Y_test, clf)
  
  ## COM O K FOLD ##
  media_kFold = fazKfold(num_folds, Xone_hot_data, clf, y)
    

  return arvore, acuracia, media_kFold

#chamada
criterio = 'entropy'
tam_teste = 0.2
num_folds = 200

arvore, acuracia, media_kfold = main(criterio, tam_teste, num_folds)
print("Acuracia com criterio:", criterio, "foi de", acuracia)
print("Média da acurácia com K_folds:", media_kfold , "tendo", num_folds, "de folds")
Image(arvore)